{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4fd44b-5240-4822-8c1e-45ab055772c9",
   "metadata": {},
   "source": [
    "<span style=color:red;font-size:55px>ASSIGNMENT</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b73af6-266d-414d-a13c-6618d119b1ea",
   "metadata": {},
   "source": [
    "<span style=color:pink;font-size:50px>REGRESSION-1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce441a-24c9-454b-b22b-8bbabaff33e1",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115346c-4cb1-4e1d-9872-a6702e7b80d2",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b951371-610c-432e-bc6a-840ebd55c5d3",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "Simple linear regression is a statistical method used to model the relationship between one independent variable (predictor) and one dependent variable (outcome). It assumes that there is a linear relationship between the independent and dependent variables. The model equation for simple linear regression is:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1x + \\epsilon \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the dependent variable.\n",
    "- \\( x \\) is the independent variable.\n",
    "- \\( \\beta_0 \\) is the intercept (the value of \\( y \\) when \\( x = 0 \\)).\n",
    "- \\( \\beta_1 \\) is the slope (the change in \\( y \\) for a unit change in \\( x \\)).\n",
    "- \\( \\epsilon \\) is the error term.\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that allows for modeling the relationship between two or more independent variables and one dependent variable. The model equation for multiple linear regression is:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the dependent variable.\n",
    "- \\( x_1, x_2, \\ldots, x_n \\) are the independent variables.\n",
    "- \\( \\beta_0 \\) is the intercept.\n",
    "- \\( \\beta_1, \\beta_2, \\ldots, \\beta_n \\) are the coefficients corresponding to each independent variable.\n",
    "- \\( \\epsilon \\) is the error term.\n",
    "\n",
    "**Example of Simple Linear Regression:**\n",
    "Suppose you want to predict the salary of employees based on their years of experience. Here, years of experience (\\( x \\)) is the independent variable, and salary (\\( y \\)) is the dependent variable. You collect data on the years of experience and the corresponding salaries of several employees. Using simple linear regression, you can build a model to predict salary based on years of experience.\n",
    "\n",
    "**Example of Multiple Linear Regression:**\n",
    "Consider a scenario where you want to predict the price of a house based on various factors such as the size of the house, the number of bedrooms, and the neighborhood's crime rate. Here, size of the house, number of bedrooms, and crime rate are independent variables, and house price is the dependent variable. Using multiple linear regression, you can build a model that takes into account all these factors to predict the house price accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4c226-7c80-405d-854b-6e5ff0f9410e",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7a915-6790-4f3b-a996-e6a2188c41a9",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29b28a-2bed-4685-ab92-7e535f7322bc",
   "metadata": {},
   "source": [
    "### Assumptions of Linear Regression\n",
    "\n",
    "Linear regression relies on several assumptions to be valid. These assumptions are crucial for the model's interpretation and predictive accuracy. Here are the key assumptions:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable is linear. This means that changes in the dependent variable are proportional to changes in the independent variables.\n",
    "\n",
    "2. **Independence of errors**: The errors (residuals) should be independent of each other. In other words, there should be no systematic patterns in the residuals.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals should be constant across all levels of the independent variables. This implies that the spread of residuals remains the same across the range of predicted values.\n",
    "\n",
    "4. **Normality of residuals**: The residuals should follow a normal distribution. This assumption is about the distribution of errors, not necessarily the distribution of the independent and dependent variables.\n",
    "\n",
    "5. **No multicollinearity**: There should be no multicollinearity among the independent variables. Multicollinearity occurs when independent variables are highly correlated with each other.\n",
    "\n",
    "### Checking Assumptions\n",
    "\n",
    "Here are some methods to check whether the assumptions of linear regression hold in a given dataset:\n",
    "\n",
    "1. **Residual plots**: Plotting the residuals against the predicted values can help identify patterns such as non-linearity and heteroscedasticity. A random scatter of residuals around zero indicates homoscedasticity.\n",
    "\n",
    "2. **Normal probability plot**: Also known as Q-Q plot, it compares the distribution of residuals to a normal distribution. If the points in the plot approximately follow a straight line, the assumption of normality is met.\n",
    "\n",
    "3. **Durbin-Watson statistic**: This statistic tests for the presence of autocorrelation in the residuals. A value around 2 suggests no autocorrelation.\n",
    "\n",
    "4. **Variance Inflation Factor (VIF)**: It measures the severity of multicollinearity in the model. A high VIF value (> 10) indicates multicollinearity.\n",
    "\n",
    "5. **Cook's distance**: It measures the influence of each observation on the regression coefficients. Large values of Cook's distance indicate influential data points that may affect the model's performance.\n",
    "\n",
    "Checking these assumptions is essential to ensure the validity and reliability of the linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1a1f3-9703-4257-a281-dcfe6b29db4e",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452150ca-7fb8-4fdd-9d91-6f495a46137f",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92729142-ae40-497d-9bf0-5afdbacd15e0",
   "metadata": {},
   "source": [
    "### Interpreting Slope and Intercept in Linear Regression\n",
    "\n",
    "In a linear regression model, the slope and intercept have specific interpretations:\n",
    "\n",
    "- **Intercept (\\( \\beta_0 \\))**: The intercept represents the value of the dependent variable when all independent variables are set to zero. It indicates the starting point of the regression line on the y-axis.\n",
    "\n",
    "- **Slope (\\( \\beta_1 \\))**: The slope represents the change in the dependent variable for a one-unit change in the independent variable. It indicates the rate of change of the dependent variable with respect to the independent variable.\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "**Scenario**: Suppose we want to predict the sales of a product based on advertising expenditure. We collect data on advertising spending (in dollars) and corresponding sales (in units) over several months.\n",
    "\n",
    "**Linear Regression Model**:\n",
    "\n",
    "\\[ \\text{Sales} = \\beta_0 + \\beta_1 \\times \\text{Advertising} + \\epsilon \\]\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "- **Intercept (\\( \\beta_0 \\))**: If the intercept (\\( \\beta_0 \\)) is 100, it means that when there is zero advertising expenditure, the expected sales are 100 units.\n",
    "\n",
    "- **Slope (\\( \\beta_1 \\))**: If the slope (\\( \\beta_1 \\)) is 0.5, it means that for every one-dollar increase in advertising spending, sales are expected to increase by 0.5 units.\n",
    "\n",
    "**Example Calculation**:\n",
    "\n",
    "- If we spend $2000 on advertising, the expected sales can be calculated as:\n",
    "  \\[ \\text{Sales} = 100 + 0.5 \\times 2000 = 1000 \\text{ units} \\]\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Interpreting the slope and intercept in a linear regression model helps us understand the relationship between the independent and dependent variables and make predictions based on the model's coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef5d6d-03c2-46a5-bd52-8db860c1b41d",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3627b3b-d6df-4d0c-a85e-c9e0a79ae376",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a52c261-d0b0-4bc0-8f8a-545e1e19058b",
   "metadata": {},
   "source": [
    "### Gradient Descent in Machine Learning\n",
    "\n",
    "**Gradient descent** is an optimization algorithm used to minimize the cost function (or loss function) of a machine learning model. It is a first-order iterative optimization algorithm that updates the parameters of the model in small steps, guided by the gradients of the cost function with respect to those parameters.\n",
    "\n",
    "### Concept of Gradient Descent\n",
    "\n",
    "1. **Objective**: Given a cost function \\( J(\\theta) \\), where \\( \\theta \\) represents the parameters of the model, the goal of gradient descent is to find the values of \\( \\theta \\) that minimize the cost function.\n",
    "\n",
    "2. **Gradient Calculation**: At each iteration of gradient descent, the algorithm computes the gradient of the cost function with respect to each parameter. The gradient points in the direction of the steepest increase of the cost function.\n",
    "\n",
    "3. **Parameter Update**: The parameters are then updated in the opposite direction of the gradient, scaled by a factor known as the learning rate (\\( \\alpha \\)). This process continues iteratively until convergence, where the change in the cost function or the parameters becomes sufficiently small.\n",
    "\n",
    "4. **Learning Rate**: The learning rate determines the size of the steps taken in the parameter space during each iteration. A small learning rate may result in slow convergence, while a large learning rate may cause oscillation or divergence.\n",
    "\n",
    "### Application in Machine Learning\n",
    "\n",
    "Gradient descent is widely used in various machine learning algorithms, including:\n",
    "\n",
    "- **Linear Regression**: It is used to find the optimal coefficients that minimize the mean squared error between the predicted and actual values.\n",
    "\n",
    "- **Logistic Regression**: It is used to find the optimal weights that minimize the log loss or cross-entropy between the predicted probabilities and actual labels.\n",
    "\n",
    "- **Neural Networks**: It is used to update the weights and biases of the neural network layers during the training process, minimizing the error between predicted and actual outputs.\n",
    "\n",
    "### Types of Gradient Descent\n",
    "\n",
    "- **Batch Gradient Descent**: It computes the gradient of the cost function using the entire training dataset at each iteration. It is computationally expensive for large datasets but guarantees convergence to the global minimum for convex cost functions.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**: It updates the parameters using only one randomly selected training sample at each iteration. It is computationally efficient but may suffer from high variance in parameter updates.\n",
    "\n",
    "- **Mini-Batch Gradient Descent**: It combines the advantages of batch and stochastic gradient descent by updating the parameters using a small subset of the training data (mini-batch) at each iteration.\n",
    "\n",
    "Gradient descent forms the backbone of many optimization algorithms in machine learning, enabling the training of models with large amounts of data and complex parameter spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf6c4d-a5e4-41d3-b66e-798705908b74",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde15d45-38cc-4768-b83b-dc28c9186871",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e46b7-4a61-431b-836f-733fffc69274",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression Model\n",
    "\n",
    "**Multiple linear regression** is an extension of simple linear regression that allows for modeling the relationship between a dependent variable and multiple independent variables. It assumes a linear relationship between the dependent variable and each independent variable, with the model equation given by:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_px_p + \\epsilon \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the dependent variable.\n",
    "- \\( x_1, x_2, \\ldots, x_p \\) are the independent variables.\n",
    "- \\( \\beta_0 \\) is the intercept.\n",
    "- \\( \\beta_1, \\beta_2, \\ldots, \\beta_p \\) are the coefficients corresponding to each independent variable.\n",
    "- \\( \\epsilon \\) is the error term.\n",
    "\n",
    "### Differences from Simple Linear Regression\n",
    "\n",
    "1. **Number of Independent Variables**: In simple linear regression, there is only one independent variable, whereas in multiple linear regression, there are two or more independent variables.\n",
    "\n",
    "2. **Model Complexity**: Multiple linear regression models are more complex than simple linear regression models due to the inclusion of multiple independent variables. This complexity allows for capturing more intricate relationships between the dependent and independent variables.\n",
    "\n",
    "3. **Interpretation of Coefficients**: In simple linear regression, there is only one coefficient (slope) associated with the independent variable. However, in multiple linear regression, each independent variable has its own coefficient, representing the change in the dependent variable for a one-unit change in that specific independent variable, holding other variables constant.\n",
    "\n",
    "4. **Model Performance**: Multiple linear regression models can potentially provide better predictions than simple linear regression models when there are multiple factors influencing the dependent variable. By including additional independent variables, the model can capture more of the variability in the dependent variable.\n",
    "\n",
    "5. **Assumptions and Diagnostics**: The assumptions and diagnostics for multiple linear regression are similar to those for simple linear regression but need to be extended to account for multiple independent variables. These include assumptions such as linearity, independence of errors, homoscedasticity, normality of residuals, and absence of multicollinearity.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose we want to predict a student's exam score based on not only the number of hours they studied (as in simple linear regression) but also their previous exam scores and the number of extracurricular activities they participate in. In this case, multiple linear regression allows us to incorporate all these factors into the prediction model, providing a more comprehensive understanding of the factors influencing the exam score.\n",
    "\n",
    "Multiple linear regression is a powerful tool for analyzing relationships between multiple variables and making predictions in various fields such as economics, finance, healthcare, and social sciences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be9e0c-51af-4208-88a8-414d3cbd69a0",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65096494-0a2a-454a-8e34-bec13d0816c9",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d90db-ec11-46d2-b6e5-e868f7573a29",
   "metadata": {},
   "source": [
    "### Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "**Multicollinearity** refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other. It can pose problems for the regression model because it violates the assumption of independence among the predictors. Multicollinearity can lead to unstable parameter estimates and inflated standard errors, making the interpretation of the coefficients unreliable.\n",
    "\n",
    "### Detection of Multicollinearity\n",
    "\n",
    "Here are some methods to detect multicollinearity in multiple linear regression:\n",
    "\n",
    "1. **Correlation Matrix**: Calculate the correlation coefficients between pairs of independent variables. High correlation coefficients (close to 1 or -1) indicate multicollinearity.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF)**: VIF measures the severity of multicollinearity by quantifying how much the variance of an estimated regression coefficient is inflated due to multicollinearity. VIF values greater than 10 or 5 are often considered indicative of multicollinearity.\n",
    "\n",
    "3. **Eigenvalues**: Calculate the eigenvalues of the correlation matrix. Small eigenvalues suggest multicollinearity.\n",
    "\n",
    "4. **Condition Number**: The condition number is the square root of the ratio of the largest eigenvalue to the smallest eigenvalue. A condition number greater than 30 indicates multicollinearity.\n",
    "\n",
    "### Addressing Multicollinearity\n",
    "\n",
    "Once multicollinearity is detected, several strategies can be employed to address this issue:\n",
    "\n",
    "1. **Feature Selection**: Remove one or more of the highly correlated variables from the model. Choose the variables that are most relevant to the outcome variable and remove redundant ones.\n",
    "\n",
    "2. **Combine Variables**: If possible, combine highly correlated variables into a single composite variable. For example, instead of including both height and weight as predictors, use Body Mass Index (BMI) as a combined measure.\n",
    "\n",
    "3. **Regularization**: Techniques like Ridge Regression and Lasso Regression introduce a penalty term to the cost function, which can help reduce the impact of multicollinearity by shrinking the coefficients of correlated variables.\n",
    "\n",
    "4. **Principal Component Analysis (PCA)**: PCA can be used to transform the original variables into a new set of uncorrelated variables (principal components) that capture most of the variation in the data. However, interpretation becomes more challenging after PCA transformation.\n",
    "\n",
    "5. **Collect More Data**: Sometimes, multicollinearity is a result of a small sample size. Collecting more data may help alleviate the problem.\n",
    "\n",
    "Addressing multicollinearity is crucial for obtaining reliable and interpretable results from multiple linear regression models. It ensures that the model accurately captures the relationships between the predictors and the outcome variable without the interference of collinear variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bb7ac-7ada-407f-a9ef-3bd91200e5dc",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b41590-838c-4607-9d61-6b32a1d2581d",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140da2b-204a-4596-ba71-64e58c591eb6",
   "metadata": {},
   "source": [
    "### Polynomial Regression Model\n",
    "\n",
    "**Polynomial regression** is a form of regression analysis in which the relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is modeled as an \\( n \\)-th degree polynomial. It extends the linear regression model by allowing for non-linear relationships between the variables. The polynomial regression model equation can be expressed as:\n",
    "\n",
    "\\[ y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots + \\beta_nx^n + \\epsilon \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the dependent variable.\n",
    "- \\( x \\) is the independent variable.\n",
    "- \\( \\beta_0, \\beta_1, \\ldots, \\beta_n \\) are the coefficients.\n",
    "- \\( \\epsilon \\) is the error term.\n",
    "\n",
    "### Differences from Linear Regression\n",
    "\n",
    "1. **Linearity**: In linear regression, the relationship between the independent and dependent variables is assumed to be linear. In polynomial regression, the relationship is modeled as a polynomial function, allowing for curved or non-linear relationships.\n",
    "\n",
    "2. **Flexibility**: Polynomial regression is more flexible than linear regression as it can capture more complex relationships between the variables. It can fit data points that follow non-linear patterns better than linear regression.\n",
    "\n",
    "3. **Degree of the Polynomial**: Polynomial regression allows for modeling relationships of various degrees (e.g., quadratic, cubic, etc.), whereas linear regression assumes a straight-line relationship between the variables.\n",
    "\n",
    "4. **Interpretation**: The interpretation of coefficients in polynomial regression becomes more complex as the degree of the polynomial increases. In linear regression, the coefficients represent the change in the dependent variable for a one-unit change in the independent variable. However, in polynomial regression, the interpretation depends on the degree of the polynomial and the specific coefficients involved.\n",
    "\n",
    "5. **Overfitting**: Polynomial regression models with high degrees of polynomials can be prone to overfitting, especially when the sample size is small. Overfitting occurs when the model captures noise in the data rather than the underlying relationship.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a scenario where you are modeling the relationship between the temperature (independent variable) and ice cream sales (dependent variable). While linear regression may assume a constant linear increase in ice cream sales with temperature, polynomial regression can capture more nuanced patterns such as an initial increase followed by a decrease in sales at very high temperatures.\n",
    "\n",
    "Polynomial regression is useful when the relationship between variables is non-linear and can provide a more accurate representation of the underlying data compared to linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563be723-4235-4c07-b5ed-b7b5381c8f2e",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f496182-7622-4017-9a11-706f65f36860",
   "metadata": {},
   "source": [
    "# Ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f0871-551f-4187-99fd-ed928e3c3879",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of Polynomial Regression\n",
    "\n",
    "#### Advantages:\n",
    "\n",
    "1. **Flexibility**: Polynomial regression can model non-linear relationships between variables more effectively than linear regression. It allows for capturing curved or complex patterns in the data.\n",
    "\n",
    "2. **Improved Fit**: In situations where the relationship between the variables is non-linear, polynomial regression can provide a better fit to the data compared to linear regression. It can minimize the residual errors and improve the overall model performance.\n",
    "\n",
    "3. **Higher Order Trends**: Polynomial regression can capture higher order trends in the data, such as quadratic or cubic relationships, which may be missed by linear regression.\n",
    "\n",
    "4. **No Requirement for Transformations**: In linear regression, transforming the variables (e.g., logarithmic transformation) may be necessary to achieve linearity. In polynomial regression, higher-order terms are included directly in the model, eliminating the need for transformations.\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "1. **Overfitting**: Polynomial regression models with high degrees of polynomials can be prone to overfitting, especially when the sample size is small. Overfitting occurs when the model captures noise in the data rather than the underlying relationship, leading to poor generalization to new data.\n",
    "\n",
    "2. **Complexity**: Polynomial regression models with higher degrees of polynomials can become complex and difficult to interpret. The interpretation of coefficients becomes more challenging as the degree of the polynomial increases.\n",
    "\n",
    "3. **Extrapolation**: Extrapolating beyond the range of observed data in polynomial regression can lead to unreliable predictions. High-degree polynomials may produce unrealistic predictions outside the observed range.\n",
    "\n",
    "### Situations for Using Polynomial Regression\n",
    "\n",
    "1. **Non-Linear Relationships**: When the relationship between the independent and dependent variables is non-linear, polynomial regression may be preferred over linear regression. For example, in modeling growth curves, sales trends, or temperature effects.\n",
    "\n",
    "2. **Curved Patterns**: When the data exhibits curved or complex patterns that cannot be adequately captured by a straight line, polynomial regression can provide a better fit to the data.\n",
    "\n",
    "3. **Limited Transformations**: When transforming variables to achieve linearity in linear regression is not desirable or feasible, polynomial regression offers an alternative approach by directly modeling the non-linear relationship.\n",
    "\n",
    "4. **Higher Order Trends**: When there is prior knowledge or evidence suggesting the presence of higher-order trends in the data, polynomial regression can be used to capture these trends effectively.\n",
    "\n",
    "In summary, polynomial regression is advantageous for modeling non-linear relationships and capturing complex patterns in the data. However, it should be used judiciously to avoid overfitting and ensure the model's interpretability and generalization to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2cf8d-b5c5-43ce-8287-92b36dadda9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
